{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DataCooker (Concepts)","text":"<p>DataCooker is a small recipe engine for biomolecular data. You describe what you want to build (targets) and how to compute them (instructions), and DataCooker resolves dependencies, executes steps, and keeps intermediate results in a cache.</p>"},{"location":"#core-concepts","title":"Core concepts","text":"<ul> <li>RecipeBook: ordered steps that declare targets, an instruction callable, and inputs (args/kwargs/params) pointing to other targets.</li> <li>ParsingCache: keyed store for intermediate and final values; supports custom key transforms and flattening.</li> <li>Cooker: orchestrates dependency resolution and execution, expanding wildcards, detecting cycles, and writing outputs back to the cache.</li> </ul>"},{"location":"#execution-flow","title":"Execution flow","text":"<ol> <li>Prep the cache with initial fields (<code>cooker.prep(data_dict)</code>).</li> <li>Resolve dependencies for requested targets (including wildcard inputs and optional targets).</li> <li>Cook instructions in order.</li> <li>Serve requested targets or defaults from the recipe book.</li> </ol>"},{"location":"#what-you-can-build","title":"What you can build","text":"<ul> <li>Parse A3M/FASTA alignments, mmCIF/CCD files, and package graph data into LMDBs using the provided recipe books.</li> <li>Combine built-in instructions with your own to assemble custom pipelines tailored to new datasets.</li> </ul>"},{"location":"#declaring-steps-example","title":"Declaring steps (example)","text":"<pre><code>from datacooker import RecipeBook\n\nbook = RecipeBook()\nbook.add(\n    targets=((\"features\", dict),),\n    instruction=lambda seq: {\"len\": len(seq)},\n    inputs={\"args\": ((\"sequence\", str),)},\n)\nbook.add(\n    targets=((\"summary\", str),),\n    instruction=lambda feats: f\"length={feats['len']}\",\n    inputs={\"args\": ((\"features\", dict),)},\n)\n</code></pre>"},{"location":"#error-handling","title":"Error handling","text":"<ul> <li>Cycles raise <code>RuntimeError</code>; missing targets raise <code>RecipeError</code>.</li> <li>Type mismatches in multi-target returns raise <code>ValueError</code> with expected arity.</li> <li>Duplicate target declarations also raise <code>ValueError</code>.</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Getting Started: install from GitHub and run a first recipe.</li> <li>Tutorials: minimal custom recipe and packaged pipeline examples.</li> <li>API: Python API reference for <code>datacooker</code>.</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section documents the public surface of <code>datacooker</code>. Module paths are relative to <code>src/datacooker</code>.</p>"},{"location":"api/#corepy","title":"<code>core.py</code>","text":"<ul> <li><code>ParsingCache(key_transform: Callable[[str], tuple[str, ...]] | None = None)</code>: nested, transformable key-value store for intermediate data. Methods:</li> <li><code>add_data(name, data)</code>: insert if not already present.</li> <li><code>__contains__(name)</code>: membership check.</li> <li><code>__getitem__(name)</code>: retrieve stored value.</li> <li><code>keys()</code>: flattened list of stored keys.</li> <li><code>RecipeBook</code>: builder for ordered recipe steps.</li> <li><code>add(targets, instruction, inputs)</code>: register a step. <code>targets</code> can be a tuple of <code>(name, type)</code> pairs or a list of such tuples. <code>inputs</code> supports <code>args</code>, <code>kwargs</code>, and static <code>params</code>.</li> <li><code>__contains__(target_name)</code>: whether a target is defined.</li> <li><code>__getitem__(target_name)</code>: retrieve the <code>Recipe</code> for a given target.</li> <li><code>targets()</code>: list declared targets.</li> <li><code>Cooker(parse_cache, recipebook, targets=None)</code>: orchestrator.</li> <li><code>prep(data_dict, fields=None)</code>: seed the cache.</li> <li><code>cook()</code>: execute all targets in dependency order.</li> <li><code>serve(targets=None)</code>: return selected targets (or defaults from the recipe book).</li> <li><code>parse(recipe_path, file_path, load_func, transform_func=None, targets=None, **extra_kwargs)</code>: helper to load a file, run the recipe on it, and return results.</li> <li><code>rebuild(recipe_path, datadict, transform_func=None, targets=None, **extra_kwargs)</code>: same as <code>parse</code> but starts from an existing <code>datadict</code>.</li> </ul>"},{"location":"api/#recipepy","title":"<code>recipe.py</code>","text":"<ul> <li><code>Variable(name, type)</code>: typed target definition.</li> <li><code>Inputs(args=(), kwargs={}, params={})</code>: inputs container used inside <code>Recipe</code>.</li> <li><code>Recipe(targets, instruction, inputs)</code>: immutable step description.</li> <li><code>RecipeError</code>: raised when a target is missing.</li> </ul>"},{"location":"api/#patterns","title":"Patterns","text":"<ul> <li>Optional targets: declare a target type as <code>type | None</code> so the cooker returns <code>None</code> when missing instead of raising.</li> <li>Wildcard args: include glob characters (e.g., <code>\"input_*\"</code>) in <code>args</code> to gather all matching cache entries.</li> <li>Multiple outputs: when a step defines multiple targets, the instruction must return a tuple with the same arity.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation-github-repo","title":"Installation (GitHub repo)","text":"<ul> <li>Requirements: Python 3.10+.</li> <li>Clone the repository:   <pre><code>git clone https://github.com/psk6950/DataCooker.git\ncd DataCooker\n</code></pre></li> <li>Install (editable) for development:   <pre><code>pip install -e .\n</code></pre></li> <li>Or use the included <code>pixi</code> environment:   <pre><code>pixi install\npixi shell\n</code></pre></li> </ul>"},{"location":"getting-started/#quick-smoke-test","title":"Quick smoke test","text":"<p>Confirm the engine runs: <pre><code>python - &lt;&lt;'PY'\nfrom datacooker import ParsingCache, RecipeBook, Cooker\nbook = RecipeBook()\nbook.add(targets=((\"x\", int),), instruction=lambda: 1, inputs={})\ncache = ParsingCache()\ncooker = Cooker(cache, book)\ncooker.prep({})\ncooker.cook()\nprint(\"x =\", cooker.serve([\"x\"]))\nPY\n</code></pre></p>"},{"location":"getting-started/#project-layout","title":"Project layout","text":"<ul> <li><code>src/datacooker</code>: core engine (<code>Cooker</code>, <code>RecipeBook</code>, <code>ParsingCache</code>, helpers)</li> <li><code>pipelines/recipe</code>: reusable recipe books (A3M/FASTA, mmCIF/CCD, LMDB builders, graph splits)</li> <li><code>pipelines/instructions</code>: atomic instruction functions used by the recipe books</li> <li><code>pipelines/utils</code> and <code>pipelines/transforms</code>: shared helpers for conversion and transforms</li> <li><code>configs</code>: Hydra/OmegaConf configs for pipeline runs</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Follow the Tutorials to run a custom recipe and a packaged pipeline.</li> <li>Review Concepts for dependency resolution details.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Pipeline tutorials live here. Each recipe under <code>pipelines/recipe/</code> has its own short guide covering inputs, targets, and execution steps.</p>"},{"location":"tutorials/#library-overview","title":"Library overview","text":"<ul> <li>A3M recipe: parse A3M headers/sequences and build an MSA container.</li> <li>AF3 training LMDB: filter CIFMol entries, attach metadata, and remove signal peptides.</li> <li>Build sequence hash map: create a FASTA hash map from CIFMol-derived sequences.</li> <li>CCD recipe: parse chem comp/atom/bond tables into a clean CCD dict.</li> <li>CIF recipe: full mmCIF ingestion to assembly/contact graphs plus metadata.</li> <li>Extract FASTA: derive FASTA strings from CIFMol.</li> <li>Filter A3M LMDB: trim residue/chain feature containers for AF3.</li> <li>Graph LMDB: build graph bytes per CIFMol with clustering context.</li> <li>Graph LMDB (attached): same as above for attached CIFMol inputs.</li> <li>Load metadata: load seq/cluster mappings and SignalP outputs.</li> <li>Sequence clustering: split FASTA, cluster proteins/antibodies, and write cluster maps.</li> <li>Train/valid graph split: build graphs, split train/valid edges, and compute stats.</li> </ul> <p>Use these in order or jump to the pipeline you need. All pages share the same format (purpose, inputs, outputs, steps, usage) for consistency.</p>"},{"location":"tutorials/a3m_recipe_book/","title":"A3M Recipe","text":""},{"location":"tutorials/a3m_recipe_book/#purpose","title":"Purpose","text":"<p>Parse A3M headers/sequences and build an MSA container for downstream feature extraction.</p>"},{"location":"tutorials/a3m_recipe_book/#inputs","title":"Inputs","text":"<ul> <li><code>raw_sequences</code> (<code>str | None</code>): raw A3M sequence block.</li> <li><code>a3m_type</code> (<code>str | None</code>): optional tag describing the A3M source/type.</li> <li><code>headers</code> (<code>list[str] | None</code>): header lines aligned to the sequences.</li> </ul>"},{"location":"tutorials/a3m_recipe_book/#outputs","title":"Outputs","text":"<ul> <li><code>parsed_sequences</code> (<code>str</code>): cleaned/parsed sequences.</li> <li><code>parsed_headers</code> (<code>dict</code>): parsed header fields keyed by sequence.</li> <li><code>msa_container</code> (<code>dict</code>): combined sequences + headers container.</li> </ul>"},{"location":"tutorials/a3m_recipe_book/#steps","title":"Steps","text":"<ol> <li><code>parse_sequence</code> \u2192 <code>parsed_sequences</code>.</li> <li><code>parse_headers</code> \u2192 <code>parsed_headers</code>.</li> <li><code>build_container</code> merges sequences and headers into <code>msa_container</code>.</li> </ol>"},{"location":"tutorials/a3m_recipe_book/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/a3m_recipe_book.py\"),\n    file_path=Path(\"data/sample.a3m\"),\n    load_func=lambda p: {\"raw_sequences\": p.read_text(), \"headers\": []},\n    targets=[\"msa_container\"],\n)\nmsa_container = results[\"msa_container\"]\n</code></pre>"},{"location":"tutorials/build_af3_training/","title":"AF3 Training LMDB","text":""},{"location":"tutorials/build_af3_training/#purpose","title":"Purpose","text":"<p>Filter CIFMol records for AF3 training, attach metadata, and drop signal peptides before packaging.</p>"},{"location":"tutorials/build_af3_training/#inputs","title":"Inputs","text":"<ul> <li><code>cifmol</code> (<code>CIFMol</code>): base CIF molecule entry.</li> <li><code>seq2seqID</code> (<code>dict</code>): sequence \u2192 seqID mapping.</li> <li><code>seqID2clusterID</code> (<code>dict</code>): seqID \u2192 clusterID mapping.</li> <li><code>signalp_dict</code> (<code>dict</code>): SignalP predictions keyed by seqID.</li> </ul>"},{"location":"tutorials/build_af3_training/#outputs","title":"Outputs","text":"<ul> <li><code>cifmol_filtered_by_resolution_date</code> (<code>CIFMol</code>): entries passing resolution/date thresholds.</li> <li><code>cifmol_no_water</code> (<code>CIFMol</code>): water-filtered version.</li> <li><code>cifmol_attached</code> (<code>CIFMolAttached</code>): CIFMol with metadata attached.</li> <li><code>cifmol_dict</code> (<code>dict</code>): final dict filtered by SignalP.</li> </ul>"},{"location":"tutorials/build_af3_training/#steps","title":"Steps","text":"<ol> <li><code>filter_by_resolution_and_date</code> (\u22649.0\u00c5, deposited by 2021-09-30) \u2192 <code>cifmol_filtered_by_resolution_date</code>.</li> <li><code>filter_water</code> removes water \u2192 <code>cifmol_no_water</code>.</li> <li><code>attach_metadata</code> adds seq/cluster info \u2192 <code>cifmol_attached</code>.</li> <li><code>filter_signalp</code> drops SignalP-positive entries \u2192 <code>cifmol_dict</code>.</li> </ol>"},{"location":"tutorials/build_af3_training/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/build_af3_training.py\"),\n    file_path=Path(\"data/sample.cif\"),  # loader must yield cifmol + metadata dicts\n    load_func=lambda p: {\n        \"cifmol\": load_cifmol(p),  # user-provided loader\n        \"seq2seqID\": {...},\n        \"seqID2clusterID\": {...},\n        \"signalp_dict\": {...},\n    },\n    targets=[\"cifmol_dict\"],\n)\n</code></pre>"},{"location":"tutorials/build_metadata/","title":"Build Metadata (Placeholder)","text":""},{"location":"tutorials/build_metadata/#purpose","title":"Purpose","text":"<p>This recipe file exists in <code>pipelines/recipe/build_metadata.py</code> but currently contains no implementation (empty file). No targets or steps are defined yet.</p>"},{"location":"tutorials/build_metadata/#inputs-outputs","title":"Inputs / Outputs","text":"<ul> <li>None defined. Once implemented, expect metadata loading/transformation steps similar to <code>load_metadata.py</code>.</li> </ul>"},{"location":"tutorials/build_metadata/#status","title":"Status","text":"<p>Use <code>load_metadata</code> for current metadata loading needs. When <code>build_metadata.py</code> gains content, update this page with its inputs, outputs, and steps.</p>"},{"location":"tutorials/build_seq_hash_map/","title":"Build Sequence Hash Map","text":""},{"location":"tutorials/build_seq_hash_map/#purpose","title":"Purpose","text":"<p>Create a hash map from FASTA entries derived from CIFMol to support clustering/deduplication.</p>"},{"location":"tutorials/build_seq_hash_map/#inputs","title":"Inputs","text":"<ul> <li><code>fasta_dict</code> (<code>dict | None</code>): mapping of identifiers to FASTA strings.</li> </ul>"},{"location":"tutorials/build_seq_hash_map/#outputs","title":"Outputs","text":"<ul> <li><code>seq_hash_map</code> (<code>dict</code>): hash \u2192 FASTA mapping.</li> </ul>"},{"location":"tutorials/build_seq_hash_map/#steps","title":"Steps","text":"<ol> <li><code>build_seq_hash_map</code> consumes <code>fasta_dict</code> and returns <code>seq_hash_map</code>.</li> </ol>"},{"location":"tutorials/build_seq_hash_map/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/build_seq_hash_map.py\"),\n    file_path=Path(\"data/fasta.json\"),  # loader should provide fasta_dict\n    load_func=lambda p: {\"fasta_dict\": load_fasta_dict(p)},  # user-provided helper\n    targets=[\"seq_hash_map\"],\n)\nseq_hash_map = results[\"seq_hash_map\"]\n</code></pre>"},{"location":"tutorials/ccd_recipe_book/","title":"CCD Recipe","text":""},{"location":"tutorials/ccd_recipe_book/#purpose","title":"Purpose","text":"<p>Parse CCD chem comp tables into a clean dictionary of components, atoms, and bonds.</p>"},{"location":"tutorials/ccd_recipe_book/#inputs","title":"Inputs","text":"<ul> <li><code>_chem_comp</code> (<code>str | None</code>): raw <code>_chem_comp</code> table.</li> <li><code>_chem_comp_atom</code> (<code>str | None</code>): raw <code>_chem_comp_atom</code> table.</li> <li><code>_chem_comp_bond</code> (<code>str | None</code>): raw <code>_chem_comp_bond</code> table.</li> </ul>"},{"location":"tutorials/ccd_recipe_book/#outputs","title":"Outputs","text":"<ul> <li><code>_chem_comp_dict</code>, <code>_chem_comp_atom_dict</code>, <code>_chem_comp_bond_dict</code> (<code>dict</code>): trimmed tables with selected columns.</li> <li><code>chem_comp_dict</code> (<code>dict</code>): parsed chem comp records (hydrogens removed, unwrapped).</li> </ul>"},{"location":"tutorials/ccd_recipe_book/#steps","title":"Steps","text":"<ol> <li><code>get_smaller_dict</code> extracts selected columns from chem comp/atom/bond tables.</li> <li><code>parse_chem_comp</code> joins the trimmed tables into <code>chem_comp_dict</code> (removes hydrogens, unwraps bonds).</li> </ol>"},{"location":"tutorials/ccd_recipe_book/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/ccd_recipe_book.py\"),\n    file_path=Path(\"data/ccd.cif\"),  # loader should yield raw CCD tables\n    load_func=load_ccd_tables,       # user-provided: returns dicts for three tables\n    targets=[\"chem_comp_dict\"],\n)\nchem_comp = results[\"chem_comp_dict\"]\n</code></pre>"},{"location":"tutorials/cif_recipe_book/","title":"CIF Recipe","text":""},{"location":"tutorials/cif_recipe_book/#purpose","title":"Purpose","text":"<p>Ingest an mmCIF file, extract metadata, normalize chem comp/entity/atom tables, and build assembly/contact graph dictionaries.</p>"},{"location":"tutorials/cif_recipe_book/#inputs","title":"Inputs","text":"<ul> <li>mmCIF category dicts: <code>_entry.id</code>, <code>_pdbx_database_status.recvd_initial_deposition_date</code>, <code>_refine.ls_d_res_high</code>, <code>_em_3d_reconstruction.resolution</code>.</li> <li>Chem comp tables: <code>_chem_comp</code>, <code>_chem_comp_atom</code>, <code>_chem_comp_bond</code>.</li> <li>Sequence/scheme tables: <code>_pdbx_poly_seq_scheme</code>, <code>_pdbx_nonpoly_scheme</code>, <code>_pdbx_branch_scheme</code>.</li> <li>Atom and entity tables: <code>_atom_site</code>, <code>_entity</code>, <code>_entity_poly</code>, <code>_entity_poly_seq</code>, <code>_pdbx_entity_nonpoly</code>, <code>_pdbx_entity_branch_descriptor</code>, <code>_pdbx_entity_branch_link</code>, <code>_pdbx_entity_branch_list</code>.</li> <li>Assembly/connection tables: <code>_pdbx_struct_oper_list</code>, <code>_pdbx_struct_assembly_gen</code>, <code>_struct_conn</code>.</li> <li>Optional: <code>ccd_db_path</code> (<code>Path</code>) to compare chem comp entries against CCD.</li> </ul>"},{"location":"tutorials/cif_recipe_book/#outputs","title":"Outputs","text":"<ul> <li><code>metadata_dict</code> (<code>dict</code>): id, deposition date, resolution.</li> <li><code>chem_comp_dict</code> / <code>chem_comp_full_dict</code> (<code>dict</code>): parsed chem comp tables, optionally compared against CCD.</li> <li><code>entity_dict</code> (<code>dict</code>): merged polymer/nonpoly/branched entities.</li> <li><code>asym_dict</code> (<code>dict</code>): asym dictionaries with atom sites and entity attachment.</li> <li><code>assembly_dict</code> (<code>dict | None</code>): assemblies with contact graph edges.</li> </ul>"},{"location":"tutorials/cif_recipe_book/#steps","title":"Steps","text":"<ol> <li>Extract single-value metadata (deposition date, resolution) \u2192 <code>metadata_dict</code>.</li> <li>Trim raw mmCIF tables to smaller dicts (chem comp, schemes, atom sites, entities, assemblies, struct_conn).</li> <li>Parse chem comp and compare against CCD (if <code>ccd_db_path</code> provided).</li> <li>Merge entity dictionaries (polymer/nonpoly/branched) and attach to asym schemes.</li> <li>Clean atom sites/struct_conn, rearrange atoms, and build full-length asym dict.</li> <li>Parse assembly + struct_oper, then build assemblies and extract contact graphs \u2192 <code>assembly_dict</code>.</li> </ol>"},{"location":"tutorials/cif_recipe_book/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\nfrom pipelines.utils.convert import load_cif  # loader -&gt; dict of mmCIF category tables\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/cif_recipe_book.py\"),\n    file_path=Path(\"data/example.cif\"),\n    load_func=load_cif,\n    targets=[\"assembly_dict\", \"metadata_dict\"],\n)\nassembly = results[\"assembly_dict\"]\nmeta = results[\"metadata_dict\"]\n</code></pre>"},{"location":"tutorials/extract_fasta/","title":"Extract FASTA","text":""},{"location":"tutorials/extract_fasta/#purpose","title":"Purpose","text":"<p>Generate FASTA strings from <code>CIFMol</code> records.</p>"},{"location":"tutorials/extract_fasta/#inputs","title":"Inputs","text":"<ul> <li><code>cifmol</code> (<code>CIFMol | None</code>): CIF molecule entry to extract sequences from.</li> </ul>"},{"location":"tutorials/extract_fasta/#outputs","title":"Outputs","text":"<ul> <li><code>fasta</code> (<code>str</code>): FASTA-formatted text.</li> </ul>"},{"location":"tutorials/extract_fasta/#steps","title":"Steps","text":"<ol> <li><code>build_fasta</code> reads sequences from <code>cifmol</code> and emits a single FASTA string.</li> </ol>"},{"location":"tutorials/extract_fasta/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\nfrom pipelines.utils.convert import load_cifmol  # user-provided loader returning {\"cifmol\": CIFMol}\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/extract_fasta.py\"),\n    file_path=Path(\"data/example.cif\"),\n    load_func=load_cifmol,\n    targets=[\"fasta\"],\n)\nfasta_text = results[\"fasta\"]\n</code></pre>"},{"location":"tutorials/filter_a3m_lmdb/","title":"Filter A3M LMDB","text":""},{"location":"tutorials/filter_a3m_lmdb/#purpose","title":"Purpose","text":"<p>Trim residue/chain feature containers from an A3M LMDB to a manageable depth for AF3 training.</p>"},{"location":"tutorials/filter_a3m_lmdb/#inputs","title":"Inputs","text":"<ul> <li><code>_residue_container</code> (<code>FeatureContainer</code>): residue-level features.</li> <li><code>_chain_container</code> (<code>FeatureContainer</code>): chain-level features.</li> </ul>"},{"location":"tutorials/filter_a3m_lmdb/#outputs","title":"Outputs","text":"<ul> <li><code>residue_container</code> (<code>FeatureContainer</code>): filtered residue features.</li> <li><code>chain_container</code> (<code>FeatureContainer</code>): filtered chain features.</li> </ul>"},{"location":"tutorials/filter_a3m_lmdb/#steps","title":"Steps","text":"<ol> <li><code>filter_a3m</code> applies max MSA depth (16,384) to both containers.</li> </ol>"},{"location":"tutorials/filter_a3m_lmdb/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\nfrom biomol.core import FeatureContainer\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/filter_a3m_lmdb.py\"),\n    file_path=Path(\"data/a3m.lmdb\"),  # loader should yield _residue_container/_chain_container\n    load_func=lambda p: {\n        \"_residue_container\": FeatureContainer(...),\n        \"_chain_container\": FeatureContainer(...),\n    },\n    targets=[\"residue_container\", \"chain_container\"],\n)\n</code></pre>"},{"location":"tutorials/graph_lmdb/","title":"Graph LMDB","text":""},{"location":"tutorials/graph_lmdb/#purpose","title":"Purpose","text":"<p>Build graph byte blobs per <code>CIFMol</code> with clustering context for LMDB storage.</p>"},{"location":"tutorials/graph_lmdb/#inputs","title":"Inputs","text":"<ul> <li><code>cifmol</code> (<code>list[CIFMol] | None</code>): list of CIFMol entries.</li> <li><code>seq_to_seq_hash_list</code> (<code>dict | None</code>): sequence \u2192 list of sequence hashes.</li> <li><code>seq_hash_to_cluster</code> (<code>dict | None</code>): sequence hash \u2192 cluster identifier.</li> </ul>"},{"location":"tutorials/graph_lmdb/#outputs","title":"Outputs","text":"<ul> <li><code>graph_bytes</code> (<code>dict[str, bytes]</code>): serialized graphs keyed by identifier.</li> </ul>"},{"location":"tutorials/graph_lmdb/#steps","title":"Steps","text":"<ol> <li><code>extract_graph_per_cifmol</code> walks each <code>cifmol</code>, adds cluster context, and emits serialized graphs.</li> </ol>"},{"location":"tutorials/graph_lmdb/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/graph_lmdb.py\"),\n    file_path=Path(\"data/cifmol.pkl\"),  # loader should provide cifmol list + clustering dicts\n    load_func=load_cifmol_with_clusters,  # user-provided\n    targets=[\"graph_bytes\"],\n)\ngraphs = results[\"graph_bytes\"]\n</code></pre>"},{"location":"tutorials/graph_lmdb_from_attached/","title":"Graph LMDB (Attached)","text":""},{"location":"tutorials/graph_lmdb_from_attached/#purpose","title":"Purpose","text":"<p>Serialize graphs from already-attached <code>CIFMol</code> entries (metadata included) into byte blobs for LMDB.</p>"},{"location":"tutorials/graph_lmdb_from_attached/#inputs","title":"Inputs","text":"<ul> <li><code>cifmol</code> (<code>list[CIFMol] | None</code>): list of attached CIFMol entries (with metadata).</li> </ul>"},{"location":"tutorials/graph_lmdb_from_attached/#outputs","title":"Outputs","text":"<ul> <li><code>graph_bytes</code> (<code>dict[str, bytes]</code>): serialized graphs keyed by identifier.</li> </ul>"},{"location":"tutorials/graph_lmdb_from_attached/#steps","title":"Steps","text":"<ol> <li><code>extract_graph_per_cifmol_attached</code> processes each <code>cifmol</code> and emits serialized graphs.</li> </ol>"},{"location":"tutorials/graph_lmdb_from_attached/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/graph_lmdb_from_attached.py\"),\n    file_path=Path(\"data/cifmol_attached.pkl\"),\n    load_func=lambda p: {\"cifmol\": load_attached_cifmol_list(p)},  # user-provided\n    targets=[\"graph_bytes\"],\n)\n</code></pre>"},{"location":"tutorials/load_metadata/","title":"Load Metadata","text":""},{"location":"tutorials/load_metadata/#purpose","title":"Purpose","text":"<p>Load sequence/cluster mapping TSVs and SignalP predictions for downstream filtering and attachment.</p>"},{"location":"tutorials/load_metadata/#inputs","title":"Inputs","text":"<ul> <li><code>seqID2seq_path</code> (<code>Path</code>): TSV mapping seqID \u2192 sequence.</li> <li><code>clusterID2seqID_path</code> (<code>Path</code>): TSV mapping clusterID \u2192 comma-separated seqIDs.</li> <li><code>signalp_dir</code> (<code>Path | None</code>): directory containing SignalP outputs.</li> </ul>"},{"location":"tutorials/load_metadata/#outputs","title":"Outputs","text":"<ul> <li><code>seqID2seq</code> (<code>dict</code>), <code>clusterID2seqID</code> (<code>dict</code>): raw mappings.</li> <li><code>seq2seqID</code> (<code>dict</code>), <code>seqID2clusterID</code> (<code>dict</code>): reversed mappings.</li> <li><code>signalp_dict</code> (<code>dict</code>): SignalP predictions keyed by seqID.</li> </ul>"},{"location":"tutorials/load_metadata/#steps","title":"Steps","text":"<ol> <li><code>load_tsv</code> reads seqID\u2192seq and clusterID\u2192seqID TSVs.</li> <li><code>reverse_dict</code> flips both mappings to get <code>seq2seqID</code> and <code>seqID2clusterID</code>.</li> <li><code>load_signalp</code> reads SignalP outputs from <code>signalp_dir</code>.</li> </ol>"},{"location":"tutorials/load_metadata/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/load_metadata.py\"),\n    file_path=Path(\".\"),  # loader can ignore and return paths below\n    load_func=lambda _: {\n        \"seqID2seq_path\": Path(\"data/seqID2seq.tsv\"),\n        \"clusterID2seqID_path\": Path(\"data/clusterID2seqID.tsv\"),\n        \"signalp_dir\": Path(\"data/signalp\"),\n    },\n    targets=[\"seq2seqID\", \"seqID2clusterID\", \"signalp_dict\"],\n)\n</code></pre>"},{"location":"tutorials/seq_cluster/","title":"Sequence Clustering","text":""},{"location":"tutorials/seq_cluster/#purpose","title":"Purpose","text":"<p>Split FASTA entries, cluster proteins/antibodies, and write consolidated cluster mappings.</p>"},{"location":"tutorials/seq_cluster/#inputs","title":"Inputs","text":"<ul> <li><code>tmp_dir</code> (<code>Path</code>): working directory for intermediate files.</li> <li><code>seq_hash_map</code> (<code>Path</code>): path to the sequence hash map file.</li> <li>(later steps) <code>fasta_path_dict</code> (<code>dict</code>): produced by the first step.</li> </ul>"},{"location":"tutorials/seq_cluster/#outputs","title":"Outputs","text":"<ul> <li><code>fasta_path_dict</code> (<code>dict</code>): paths to separated FASTA files.</li> <li><code>protein_cluster_dict</code>, <code>protein_d_cluster_dict</code>, <code>antibody_cluster_dict</code> (<code>dict</code>): clustering results.</li> <li><code>cluster_dict</code> (<code>dict</code>): consolidated cluster mapping.</li> </ul>"},{"location":"tutorials/seq_cluster/#steps","title":"Steps","text":"<ol> <li><code>separate_sequences</code> writes per-category FASTA files \u2192 <code>fasta_path_dict</code>.</li> <li><code>protein_cluster</code> clusters protein sequences \u2192 <code>protein_cluster_dict</code>, <code>protein_d_cluster_dict</code>.</li> <li><code>antibody_cluster</code> clusters antibody sequences \u2192 <code>antibody_cluster_dict</code>.</li> <li><code>write_cluster</code> merges clusters and writes final <code>cluster_dict</code>.</li> </ol>"},{"location":"tutorials/seq_cluster/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/seq_cluster.py\"),\n    file_path=Path(\".\"),  # loader should supply tmp_dir and seq_hash_map\n    load_func=lambda _: {\"tmp_dir\": Path(\"/tmp/clusters\"), \"seq_hash_map\": Path(\"data/seq_hash_map.json\")},\n    targets=[\"cluster_dict\"],\n)\nclusters = results[\"cluster_dict\"]\n</code></pre>"},{"location":"tutorials/train_valid_graph_split/","title":"Train/Valid Graph Split","text":""},{"location":"tutorials/train_valid_graph_split/#purpose","title":"Purpose","text":"<p>Build whole graphs from an edge TSV, split into train/valid components, extract edge lists, and compute statistics.</p>"},{"location":"tutorials/train_valid_graph_split/#inputs","title":"Inputs","text":"<ul> <li><code>edge_tsv_path</code> (<code>Path</code>): TSV with graph edges.</li> <li><code>ignore_nodes</code> (<code>list</code>): nodes to drop from the graph.</li> <li><code>train_ratio</code> (<code>float</code>): ratio for splitting edges into train vs valid.</li> </ul>"},{"location":"tutorials/train_valid_graph_split/#outputs","title":"Outputs","text":"<ul> <li><code>whole_graph</code>, <code>polymer_graph</code> (<code>Graph</code>): constructed graphs.</li> <li><code>subgraphs</code> (<code>dict</code>), <code>edge_wo_ligand_counts</code> (<code>dict</code>): component splits and ligand-filtered edge counts.</li> <li><code>train_edges</code>, <code>valid_edges</code> (<code>list</code>): split edge identifiers.</li> <li><code>train_edge_list</code>, <code>valid_edge_list</code> (<code>list</code>): extracted edge rows.</li> <li><code>train_edge_statistics</code>, <code>valid_edge_statistics</code> (<code>list</code>): per-split category counts.</li> </ul>"},{"location":"tutorials/train_valid_graph_split/#steps","title":"Steps","text":"<ol> <li><code>build_whole_graph</code> constructs <code>whole_graph</code> and <code>polymer_graph</code> from TSV.</li> <li><code>split_graph_by_components</code> splits into subgraphs and counts edges without ligands.</li> <li><code>split_train_valid</code> divides edges by <code>train_ratio</code>.</li> <li><code>extract_edges</code> pulls edge rows for train/valid lists.</li> <li><code>count_category_count</code> computes edge statistics for each split.</li> </ol>"},{"location":"tutorials/train_valid_graph_split/#usage","title":"Usage","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/train_valid_graph_split.py\"),\n    file_path=Path(\"data/edges.tsv\"),\n    load_func=lambda p: {\"edge_tsv_path\": p, \"ignore_nodes\": [], \"train_ratio\": 0.8},\n    targets=[\"train_edge_list\", \"valid_edge_list\", \"train_edge_statistics\", \"valid_edge_statistics\"],\n)\n</code></pre>"}]}