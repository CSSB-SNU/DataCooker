{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DataCooker","text":"<p>DataCooker is a small recipe engine for biomolecular data. You describe what you want to build (targets) and how to compute them (instructions), and DataCooker resolves dependencies, executes steps, and keeps intermediate results in a cache. The repository also ships ready-made pipelines for A3M/FASTA parsing, mmCIF/CCD processing, LMDB packaging, and graph splits used by downstream training jobs.</p>"},{"location":"#concepts-at-a-glance","title":"Concepts at a glance","text":"<ul> <li>Recipes are declared with a small DSL (<code>RecipeBook</code>) that names targets and their inputs.</li> <li>Execution is handled by the <code>Cooker</code>, which walks dependencies and persists intermediate results in a <code>ParsingCache</code>.</li> <li>You can mix built-in instructions from <code>pipelines/instructions/</code> with your own functions to assemble new pipelines quickly.</li> </ul>"},{"location":"#what-you-can-build","title":"What you can build","text":"<ul> <li>Parse alignments, mmCIF/CCD files, and package graph data into LMDBs using the provided recipe books.</li> <li>Create lightweight custom pipelines by combining targets, instructions, and transforms tailored to your data.</li> </ul>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Getting Started: install from GitHub and run a first recipe.</li> <li>Tutorials: minimal custom recipe and packaged pipeline examples.</li> <li>Concepts: dependency resolution and cache design details.</li> <li>API: Python API reference for <code>datacooker</code>.</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section documents the public surface of <code>datacooker</code>. Module paths are relative to <code>src/datacooker</code>.</p>"},{"location":"api/#corepy","title":"<code>core.py</code>","text":"<ul> <li><code>ParsingCache(key_transform: Callable[[str], tuple[str, ...]] | None = None)</code>: nested, transformable key-value store for intermediate data. Methods:</li> <li><code>add_data(name, data)</code>: insert if not already present.</li> <li><code>__contains__(name)</code>: membership check.</li> <li><code>__getitem__(name)</code>: retrieve stored value.</li> <li><code>keys()</code>: flattened list of stored keys.</li> <li><code>RecipeBook</code>: builder for ordered recipe steps.</li> <li><code>add(targets, instruction, inputs)</code>: register a step. <code>targets</code> can be a tuple of <code>(name, type)</code> pairs or a list of such tuples. <code>inputs</code> supports <code>args</code>, <code>kwargs</code>, and static <code>params</code>.</li> <li><code>__contains__(target_name)</code>: whether a target is defined.</li> <li><code>__getitem__(target_name)</code>: retrieve the <code>Recipe</code> for a given target.</li> <li><code>targets()</code>: list declared targets.</li> <li><code>Cooker(parse_cache, recipebook, targets=None)</code>: orchestrator.</li> <li><code>prep(data_dict, fields=None)</code>: seed the cache.</li> <li><code>cook()</code>: execute all targets in dependency order.</li> <li><code>serve(targets=None)</code>: return selected targets (or defaults from the recipe book).</li> <li><code>parse(recipe_path, file_path, load_func, transform_func=None, targets=None, **extra_kwargs)</code>: helper to load a file, run the recipe on it, and return results.</li> <li><code>rebuild(recipe_path, datadict, transform_func=None, targets=None, **extra_kwargs)</code>: same as <code>parse</code> but starts from an existing <code>datadict</code>.</li> </ul>"},{"location":"api/#recipepy","title":"<code>recipe.py</code>","text":"<ul> <li><code>Variable(name, type)</code>: typed target definition.</li> <li><code>Inputs(args=(), kwargs={}, params={})</code>: inputs container used inside <code>Recipe</code>.</li> <li><code>Recipe(targets, instruction, inputs)</code>: immutable step description.</li> <li><code>RecipeError</code>: raised when a target is missing.</li> </ul>"},{"location":"api/#patterns","title":"Patterns","text":"<ul> <li>Optional targets: declare a target type as <code>type | None</code> so the cooker returns <code>None</code> when missing instead of raising.</li> <li>Wildcard args: include glob characters (e.g., <code>\"input_*\"</code>) in <code>args</code> to gather all matching cache entries.</li> <li>Multiple outputs: when a step defines multiple targets, the instruction must return a tuple with the same arity.</li> </ul>"},{"location":"concepts/","title":"Concepts &amp; Architecture","text":""},{"location":"concepts/#core-pieces","title":"Core pieces","text":"<ul> <li><code>RecipeBook</code>: ordered collection of steps. Each step declares <code>targets</code>, an <code>instruction</code> callable, and <code>inputs</code> that point to other targets (args/kwargs) plus static <code>params</code>.</li> <li><code>ParsingCache</code>: a keyed store for intermediate and final values. It can flatten keys (<code>foo.bar</code>) or apply a custom <code>key_transform</code> for nested access.</li> <li><code>Cooker</code>: orchestrates execution. It resolves dependencies, handles wildcard inputs (glob patterns like <code>input*</code>), detects cycles, runs instructions, and stores outputs back into the cache.</li> <li>Helpers: <code>parse()</code> loads an input file, seeds the cache, runs a recipe book from disk, and returns selected targets. <code>rebuild()</code> is similar but starts from an in-memory <code>datadict</code>.</li> </ul>"},{"location":"concepts/#execution-model","title":"Execution model","text":"<ol> <li>Prep: seed the cache with initial fields via <code>cooker.prep(data_dict)</code>.</li> <li>Resolve: for each requested target, resolve all upstream inputs:</li> <li>Args/kwargs in <code>Inputs</code> are mapped by name to other targets.</li> <li>Wildcard args are expanded against existing cache keys.</li> <li>Optional targets (<code>type | None</code>) return <code>None</code> if not declared.</li> <li>Cook: run instructions in dependency order, caching outputs as they are produced.</li> <li>Serve: retrieve specific targets or all defaults defined by the recipe book.</li> </ol>"},{"location":"concepts/#declaring-steps","title":"Declaring steps","text":"<pre><code>from datacooker import RecipeBook\n\nbook = RecipeBook()\nbook.add(\n    targets=((\"features\", dict),),\n    instruction=lambda seq: {\"len\": len(seq)},\n    inputs={\"args\": ((\"sequence\", str),)},\n)\nbook.add(\n    targets=((\"summary\", str),),\n    instruction=lambda feats: f\"length={feats['len']}\",\n    inputs={\"args\": ((\"features\", dict),)},\n)\n</code></pre>"},{"location":"concepts/#working-with-files-vs-memory","title":"Working with files vs memory","text":"<ul> <li>Use <code>datacooker.core.parse()</code> when your starting point is a file path and you have a loader function that returns a data dictionary.</li> <li>Use <code>datacooker.core.rebuild()</code> when you already have a populated <code>datadict</code> (e.g., from a previous pipeline stage or cached LMDB read).</li> </ul>"},{"location":"concepts/#error-handling","title":"Error handling","text":"<ul> <li>Cycles raise <code>RuntimeError</code>.</li> <li>Missing targets raise <code>RecipeError</code>.</li> <li>Type mismatches in returned tuples raise <code>ValueError</code> with expected target counts.</li> <li>Duplicate targets in a recipe book raise <code>ValueError</code>.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation-github-repo","title":"Installation (GitHub repo)","text":"<ul> <li>Requirements: Python 3.10+.</li> <li>Clone the repository:   <pre><code>git clone https://github.com/psk6950/DataCooker.git\ncd DataCooker\n</code></pre></li> <li>Install (editable) for development:   <pre><code>pip install -e .\n</code></pre></li> <li>Or use the included <code>pixi</code> environment:   <pre><code>pixi install\npixi shell\n</code></pre></li> </ul>"},{"location":"getting-started/#quick-smoke-test","title":"Quick smoke test","text":"<p>Confirm the engine runs: <pre><code>python - &lt;&lt;'PY'\nfrom datacooker import ParsingCache, RecipeBook, Cooker\nbook = RecipeBook()\nbook.add(targets=((\"x\", int),), instruction=lambda: 1, inputs={})\ncache = ParsingCache()\ncooker = Cooker(cache, book)\ncooker.prep({})\ncooker.cook()\nprint(\"x =\", cooker.serve([\"x\"]))\nPY\n</code></pre></p>"},{"location":"getting-started/#project-layout","title":"Project layout","text":"<ul> <li><code>src/datacooker</code>: core engine (<code>Cooker</code>, <code>RecipeBook</code>, <code>ParsingCache</code>, helpers)</li> <li><code>pipelines/recipe</code>: reusable recipe books (A3M/FASTA, mmCIF/CCD, LMDB builders, graph splits)</li> <li><code>pipelines/instructions</code>: atomic instruction functions used by the recipe books</li> <li><code>pipelines/utils</code> and <code>pipelines/transforms</code>: shared helpers for conversion and transforms</li> <li><code>configs</code>: Hydra/OmegaConf configs for pipeline runs</li> </ul>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Follow the Tutorials to run a custom recipe and a packaged pipeline.</li> <li>Read Concepts for dependency resolution details.</li> </ul>"},{"location":"pipelines/","title":"Pipelines &amp; Recipes","text":"<p>The repository ships several recipe books under <code>pipelines/recipe</code>. Each book bundles instructions from <code>pipelines/instructions</code> into an executable workflow.</p>"},{"location":"pipelines/#data-parsing","title":"Data parsing","text":"<ul> <li><code>a3m_recipe_book.py</code>: parse A3M/FASTA alignments (headers + sequences) into biomol feature containers; see <code>pipelines/instructions/a3m_instructions.py</code>.</li> <li><code>cif_recipe_book.py</code>: mmCIF structure parsing with biomol features; uses helpers in <code>pipelines/instructions/cif_instructions.py</code>.</li> <li><code>ccd_recipe_book.py</code>: ingest CCD (chemical component dictionary) records.</li> <li><code>extract_fasta.py</code>: extract FASTA from alignment/sequence sources.</li> </ul>"},{"location":"pipelines/#packaging-and-metadata","title":"Packaging and metadata","text":"<ul> <li><code>filter_a3m_lmdb.py</code>: filter A3M datasets and store them in LMDB.</li> <li><code>build_seq_hash_map.py</code>: build hash maps for sequence deduplication.</li> <li><code>build_metadata.py</code>, <code>load_metadata.py</code>: enrich and load dataset metadata.</li> <li><code>build_af3_training.py</code>: assemble inputs for AF3 training.</li> </ul>"},{"location":"pipelines/#graph-pipelines","title":"Graph pipelines","text":"<ul> <li><code>graph_lmdb.py</code>, <code>graph_lmdb_from_attached.py</code>: convert graph-like biomolecular data into LMDB shards.</li> <li><code>seq_cluster.py</code>: build sequence clusters.</li> <li><code>train_valid_graph_split.py</code>: construct whole graphs, split them, extract edge lists, and compute edge statistics.</li> </ul>"},{"location":"pipelines/#running-a-recipe-book","title":"Running a recipe book","text":"<pre><code>from pathlib import Path\nfrom datacooker.core import parse\nfrom pipelines.utils.convert import load_cif  # loader -&gt; dict[str, Any]\n\nresults = parse(\n    recipe_path=Path(\"pipelines/recipe/cif_recipe_book.py\"),\n    file_path=Path(\"data/example.cif\"),\n    load_func=load_cif,\n    targets=[\"structure_container\"],  # or None for all\n)\n</code></pre>"},{"location":"pipelines/#customizing","title":"Customizing","text":"<ul> <li>Swap instruction functions to change preprocessing.</li> <li>Add targets with <code>RecipeBook.add()</code> to attach more derived signals.</li> <li>Use wildcard args when a step should consume multiple cached keys (e.g., <code>\"chain_*\"</code>).</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section shows how to use DataCooker in practice, from a tiny recipe to a packaged pipeline.</p>"},{"location":"tutorials/#1-minimal-custom-recipe","title":"1. Minimal custom recipe","text":"<p>Define a two-step recipe, load it into a cache-backed cooker, and serve results: <pre><code>from datacooker import ParsingCache, RecipeBook, Cooker\n\nbook = RecipeBook()\nbook.add(\n    targets=((\"clean_text\", str),),\n    instruction=lambda raw: raw.strip().upper(),\n    inputs={\"args\": ((\"raw_text\", str),)},\n)\nbook.add(\n    targets=((\"length\", int),),\n    instruction=lambda text: len(text),\n    inputs={\"args\": ((\"clean_text\", str),)},\n)\n\ncache = ParsingCache()\ncooker = Cooker(cache, book)\ncooker.prep({\"raw_text\": \"  ACDEfg  \"})\ncooker.cook()\nprint(cooker.serve([\"clean_text\", \"length\"]))\n</code></pre></p>"},{"location":"tutorials/#2-run-a-packaged-pipeline","title":"2. Run a packaged pipeline","text":"<p>The repository ships reusable recipe books under <code>pipelines/recipe</code>. To run one, pick a recipe file and call the helper <code>parse</code> API: <pre><code>from pathlib import Path\nfrom datacooker.core import parse\nfrom pipelines.utils.convert import load_cif  # choose a loader for your data\n\nrecipe_path = Path(\"pipelines/recipe/train_valid_graph_split.py\")\nresults = parse(\n    recipe_path=recipe_path,\n    file_path=Path(\"data/edges.tsv\"),\n    load_func=load_cif,\n    transform_func=None,\n    targets=[\"train_edge_list\", \"valid_edge_list\"],\n)\nprint(results)\n</code></pre></p> <p>Common recipe books to explore: - <code>pipelines/recipe/a3m_recipe_book.py</code>: A3M/FASTA alignments to biomolecular feature containers - <code>pipelines/recipe/cif_recipe_book.py</code>, <code>pipelines/recipe/ccd_recipe_book.py</code>: mmCIF/CCD extraction - <code>pipelines/recipe/graph_lmdb.py</code>, <code>pipelines/recipe/graph_lmdb_from_attached.py</code>: LMDB builders for graph data - <code>pipelines/recipe/train_valid_graph_split.py</code>: graph splits with basic stats</p>"},{"location":"tutorials/#3-iterate-on-your-own-recipe","title":"3. Iterate on your own recipe","text":"<p>Use the same pattern as the minimal example: declare targets and inputs, add the instruction, and call <code>cooker.prep/cook/serve</code>. Keep recipe files alongside your data project or inside <code>pipelines/recipe/</code> to reuse shared instructions and transforms.</p> <p>Next steps: - Dive into the concepts to understand dependency resolution (<code>RecipeBook</code>, <code>Cooker</code>, <code>ParsingCache</code>). - Browse <code>pipelines/instructions/</code> for reusable instruction functions.</p>"}]}